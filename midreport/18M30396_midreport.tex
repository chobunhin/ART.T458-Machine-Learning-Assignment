\documentclass{article}
\usepackage{latexsym}
\usepackage{bm}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{calrsfs}
\usepackage[all]{xy}
\usepackage[linesnumbered,boxed]{algorithm2e}
\usepackage{float}
\usepackage[usenames,dvipsnames]{color}
\usepackage{listings}
\usepackage{textcomp}
%\usepackage[T1]{fontenc}
\usepackage[Q=yes]{examplep}
\usepackage{courier}
\usepackage{subfigure}% in preamble
\usepackage{bm}
\usepackage{hyperref}

\begin{document}
\newcommand{\setr}{\mathbb{R}}
\newenvironment{myproof}[1][Proof.]{\begin{trivlist}
\item[\hskip \labelsep {\bfseries #1}]}{\end{trivlist}}


\title{Machine Learning Mid-term Assignment (Shimosaka)}
\author{18M30396 Cho Bunhin}
\date{\today} 
\maketitle
Problems done: problem 1(all), problem 3(1,2,3), problem 6(all).

MATLAB codes can be found at\newline
\small
\url{https://github.com/chobunhin/ART.T458-Machine-Learning-Assignment}
\section*{Problem 1}
\normalsize
Linear logistic regression with Batch steep descent
\begin{itemize}
	\item For $i$-th sample, corresponding function value and gradient read
		\begin{equation}
		J_i(w)=\ln(1+\exp(-y_iw^Tx_i)) ,
		\end{equation}
		\begin{equation}
		\frac{\partial J_i}{\partial w}(w)=-\frac{\exp(-y_iw^Tx_i)}{1+\exp(-y_iw^Tx_i)}y_ix_i,
		\end{equation}
	\item for $t$-th iteration, we update $w$ by a batch of samples as well as the $L^2$ penalty:
		\begin{equation}
		w^{t+1} = w^t - \eta \cdot \left(\sum_{i\in I^t}\frac{\partial J_i}{\partial w} + \lambda w^t\right).
		\end{equation}
\end{itemize}
Linear logistic regression with AdaGrad method
\begin{itemize}
	\item Diagonal Hessian 
		\begin{equation}
		  H_t = diag(G_t^{1/2}),
		\end{equation}
		where $\{G_t\}_i = \sum_{\tau=1}^{t}\{g^\tau\}_i^2$, and $g$ is the gradient of full $J(w)$.
  \item Accumulated gradient
    \begin{equation}
      d_t  = \sum_{\tau=1}^t g_{\tau}
    \end{equation}
  \item Updating rule:
  	\begin{equation}
  	  w^{t+1} = w^t - \eta_0 H_t^{-1}d_t
  	\end{equation}
\end{itemize}
MATLAB code see \verb|problem1.m|
\newline
Comparisons:
Toy Dataset II; Running spec see \verb|problem1.m|; We observed that AdaGrad method converges considerably faster than batch steepest descent method.
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{problem1_fval_comparison}
	\caption{Function values comparison: blue-batch, red-adagrad}
	\label{fig:problem1fvalcomparison}
\end{figure}
\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{problem1_result_comparison}
	\caption{Separation results comparison}
	\label{fig:problem1resultcomparison}
\end{figure}


\section*{Problem 3}
\begin{myproof}[proof of 1.]
	The original problem is equivalent to 
	\begin{equation}
	\begin{cases}\label{eq_problem_2}
	\quad \min_{w\in\setr^d, z\in \setr^n} &z^T \bf{1}+\lambda \|w\|_2^2\\
	\quad \mbox{s.t. } & z\geq 0 \\
	                   & z_i+y_i w^T x_i - 1\geq 0 \\
	\end{cases}
	\end{equation}
	with Lagrangian ($\mu\geq 0, \alpha\geq 0$):
	\begin{equation}\label{eq_lagrange}
	L(w,z;\mu,\alpha) = \langle z, \bf{1}\rangle +\lambda \langle w, w\rangle-\langle \mu, z \rangle - \langle \alpha, z \rangle - \sum_{i=1}^n (y_i \langle w, x_i\rangle -1).
	\end{equation}
	Following standard dual problem generating, we have
	\begin{eqnarray}\label{eq_w_hat}
	\frac{\partial L}{\partial w}=0 \Rightarrow \hat{w} = \frac{1}{2\lambda} \sum_{i=1}^{n}\alpha_i y_i x_i,
	\end{eqnarray}
	\begin{eqnarray}
	\frac{\partial L}{\partial z}=0 \Rightarrow \bf{1} - \mu - \alpha = 0,
	\end{eqnarray}
	and the dual objective function now reads:
	\begin{equation}
	L(\hat{w},\hat{z};\mu,\alpha)=-\frac{1}{4\lambda}\langle \sum_{i=1}^{n}\alpha_i y_i x_i, \sum_{j=1}^{n}\alpha_j y_j x_j\rangle+\langle\alpha, \bf{1}\rangle,
	\end{equation}
	which gives the conclusion that
	\begin{equation}
	K_{ij} = y_iy_j\langle x_i, x_j\rangle,
	\end{equation}
	and the dual problem reads:
	\begin{equation}
	\begin{cases}\label{eq_problem_dual}
	\quad \max_{\alpha\in \setr^n} &-\frac{1}{4\lambda}\langle\alpha,K\alpha\rangle + \langle\alpha,\bf{1}\rangle\\
	\quad \mbox{s.t. } & \bf{0}\leq \alpha\leq \bf{1} 
	\end{cases}
	\end{equation}
	\qed
\end{myproof}

\begin{myproof}[2.]
	KKT condition $\Rightarrow \frac{\partial L}{\partial w}=0\Rightarrow\hat{w} = \frac{1}{2\lambda} \sum_{i=1}^{n}\alpha_i y_i x_i$ according to (\ref{eq_w_hat}).\qed
\end{myproof}
\newpage
\begin{myproof}[3.]
	MATLAB code see \verb|problem3.m|
	\newline
	Settings: Toy Dataset II; number of samples: 60; learning rate $\eta_t = \eta = 0.005$; iterate 1000 times. The primal-dual objective values as well as final result are shown in figure \ref{fig:problem3primaldual} and figure \ref{fig:problem3result}.
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.7\linewidth]{problem3_primal_dual}
		\caption{primal dual objective value wrt. iterate}
		\label{fig:problem3primaldual}
	\end{figure}
	\begin{figure}[h!]
		\centering
		\includegraphics[width=0.7\linewidth]{problem3_result}
		\caption{result given by SVM}
		\label{fig:problem3result}
	\end{figure}
	
\end{myproof}
\section*{Problem 6}
\begin{myproof}[1. nuclear norm] For matrix $Z\in \setr^{m\times n}$, let the singular values of $Z$ be
	\begin{equation}
	\sigma_1\geq\sigma_2\geq\cdots\geq\sigma_{\min(m,n)},
	\end{equation}
	then the nuclear norm of $Z$ reads
	\begin{equation}
	\|Z\|_\ast = \sum_{j=1}^{\min(m,n)}\sigma_j.
	\end{equation}
\qed
\end{myproof}
\begin{myproof}[2. proximal operator of nuclear norm]
	Let $Z=U\Sigma V^T$ be the SVD of $Z$, where $\Sigma = diag\{\sigma_1,\sigma_2,\ldots,\sigma_{\min(m,n)}\}$. Then proximal operator
	\begin{eqnarray}
	Prox_{\lambda\|\cdot\|_\ast}(Z) &=& \arg\min_X \frac{1}{2}\|X-Z\|_F^2+\lambda\|X\|_\ast\\
	&=&U\cdot diag\{S_\lambda(\sigma_1),\ldots, S_\lambda(\sigma_{\min(m,n)})\}\cdot V^T,
	\end{eqnarray}
	where soft shrinkage $S_\lambda(\sigma)=\max(\sigma-\lambda,0)$.
	\qed
\end{myproof}
\begin{myproof}[3.]
  Proximal gradient method implementation:
	\begin{itemize}
		\item Objective function:
		  \begin{equation}
		  f(Z) = \sum_{i,j\in Q}|A_{i,j}-Z_{i,j}|^2+\lambda\|Z\|_\ast\equiv g(Z)+h(Z),
		  \end{equation}
		\item Gradient of smooth part:
		  \begin{equation}
		  \big(\nabla g(Z)\big)_{i,j}=
		  \begin{cases}
		  0 & (i,j)\not\in Q \\
		  2(Z_{i,j}-A_{i,j}) & (i,j)\in Q,
		  \end{cases}
		  \end{equation}
		  which infers that
		  $ \|\nabla g(Z_1)-\nabla g(Z_2)\|_F \leq \gamma\|Z_1-Z_2\|_F$
		  and Lipschitz constant $\gamma=2$.
		\item Updating rule:
		  \begin{align}
		  Z^{(t+1)} &= \arg \min_Z \Big\{ \langle\nabla g(Z^{(t)}), Z-Z^{(t)}\rangle + \frac{\gamma}{2}\|Z-Z^{(t)}\|_F^2+h(Z)\Big\}\\
		            &= \arg \min_Z \Big\{ \langle\nabla g(Z^{(t)}), Z-Z^{(t)}\rangle+\|Z-Z^{(t)}\|_F^2+\lambda \|Z\|_\ast\Big\}\\
		            &= \mbox{Prox}_{\frac{\lambda}{2}\|\cdot\|_\ast}\Big(Z^{(t)}-\frac{1}{2}\nabla g(Z^{(t)})\Big)
		  \end{align}
    \item Parameter tunning: $\lambda = 0.05$; 200 iterations; all-zero initial guess.
    \item For recovered low-rank matrix see figure \ref{fig:problem6resultsurfplot}.
    \begin{figure}[!h]
    	\centering
    	\includegraphics[width=0.7\linewidth]{problem6_result_surfplot}
    	\caption{Surf plot of recovered matrix given by nuclear norm regularization with proximal gradient method}
    	\label{fig:problem6resultsurfplot}
    \end{figure}
	\end{itemize}
\end{myproof}
\begin{myproof}[4.]
	Non-negative matrix factorization and comparison. MATLAB code see \verb|problem6.m|
	
	\begin{itemize}
		\item Hierarchical alternating least squares (HALS) for NMF solves
		\begin{equation}
		\min \|A-\sum_{k=1}^r u_k v_k^T\|_Q^2
		\end{equation}
		alternatively w.r.t non-negative $u,v$ and successively w.r.t subscript $k$. Each update can be solved in close form as shown below. We only give formula on $u_k$.
		\begin{align}
		  f(u_k) &= \|R_k - u_k v_k^T\|_Q^2\\
		         &= \sum_{i,j\in Q}\Big[(R_k)_{i,j}-(u_k)_i (v_k)_j\Big]^2,
		\end{align}
		$\partial f/\partial (u_k)_i=0$ yields (also considering non-negativity)
		\begin{equation}
		(u_k)_i = \max\Big(0, \frac{\sum_{i, (i,j)\in Q}(v_k)_j\cdot (R_k)_{i,j}}{\sum_{i, (i,j)\in Q} (v_k)_j^2}\Big),
		\end{equation}
		where residual matrix 
		\begin{equation}
		R_k = A-\sum_{l\neq k}u_l v_l^T.
		\end{equation}
		\item Parameter setting: rank for factorization $r=3$ (rank of $A$ is 2); 200 iterations; random initial guess with uniform dist on $[0,1]$;
		Dataset III.
		\item Relative error comparison with PG is recorded in figure \ref{fig:problem6resultcomparison}.
		\begin{figure}[h!]
			\centering
			\includegraphics[width=0.7\linewidth]{problem6_result_comparison}
			\caption{relative error comparison of HALS and PG}
			\label{fig:problem6resultcomparison}
		\end{figure}
		\item Analysis for HALS: simple updating rule (fixed-point iteration); linear convergence to true matrix; tricky initial guess since the iteration fails when $u_k$ or $v_k$ becomes zero.
		\item Analysis for PG: solving convex problem meaning convergence will not be a problem; not necessarily gives exact recovery; keep low-rank property well.
		
	\end{itemize}
\end{myproof}
\section*{Suggestion}
The pdf slides of the lectures are a little bit too large, which makes it difficult for viewing the slides in computer. Also it would be thankful if searching function is available in the slides. Many thanks to Prof. Shimosaka for giving this excellent lecture.
\end{document}



